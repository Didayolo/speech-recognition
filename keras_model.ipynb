{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Speech Recognition Challenge\n",
    "\n",
    "### Can you build an algorithm that understands simple speech commands?\n",
    "\n",
    "From here : https://www.kaggle.com/c/tensorflow-speech-recognition-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import division\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POSSIBLE_LABELS = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "id2name = {i: name for i, name in enumerate(POSSIBLE_LABELS)}\n",
    "name2id = {name: i for i, name in id2name.items()}\n",
    "len(id2name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    \"\"\" Return 2 lists of tuples:\n",
    "    [(class_id, user_id, path), ...] for train\n",
    "    [(class_id, user_id, path), ...] for validation\n",
    "    \"\"\"\n",
    "    # Just a simple regexp for paths with three groups:\n",
    "    # prefix, label, user_id\n",
    "    pattern = re.compile(\"(.+\\/)?(\\w+)\\/([^_]+)_.+wav\")\n",
    "    all_files = glob(os.path.join(data_dir, 'train/audio/*/*wav'))\n",
    "\n",
    "    with open(os.path.join(data_dir, 'train/validation_list.txt'), 'r') as fin:\n",
    "        validation_files = fin.readlines()\n",
    "    valset = set()\n",
    "    for entry in validation_files:\n",
    "        r = re.match(pattern, entry)\n",
    "        if r:\n",
    "            valset.add(r.group(3))\n",
    "\n",
    "    possible = set(POSSIBLE_LABELS)\n",
    "    train, val = [], []\n",
    "    for entry in all_files:\n",
    "        r = re.match(pattern, entry)\n",
    "        if r:\n",
    "            label, uid = r.group(2), r.group(3)\n",
    "            if label == '_background_noise_':\n",
    "                label = 'silence'\n",
    "            if label not in possible:\n",
    "                label = 'unknown'\n",
    "\n",
    "            label_id = name2id[label]\n",
    "\n",
    "            sample = (label, label_id, uid, entry)\n",
    "            if uid in valset:\n",
    "                val.append(sample)\n",
    "            else:\n",
    "                train.append(sample)\n",
    "\n",
    "    print('There are {} train and {} val samples'.format(len(train), len(val)))\n",
    "    \n",
    "    columns_list = ['label', 'label_id', 'user_id', 'wav_file']\n",
    "    \n",
    "    train_df = pd.DataFrame(train, columns = columns_list)\n",
    "    valid_df = pd.DataFrame(val, columns = columns_list)\n",
    "    \n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combien d'exemples de test ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57929 train and 6798 val samples\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df = load_data('./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>wav_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>708b8d51</td>\n",
       "      <td>./data/train/audio/yes/708b8d51_nohash_0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>db79a764</td>\n",
       "      <td>./data/train/audio/yes/db79a764_nohash_0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>c6a23ff5</td>\n",
       "      <td>./data/train/audio/yes/c6a23ff5_nohash_0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>c93d5e22</td>\n",
       "      <td>./data/train/audio/yes/c93d5e22_nohash_2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>b575b5fb</td>\n",
       "      <td>./data/train/audio/yes/b575b5fb_nohash_0.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  label_id   user_id                                      wav_file\n",
       "0   yes         0  708b8d51  ./data/train/audio/yes/708b8d51_nohash_0.wav\n",
       "1   yes         0  db79a764  ./data/train/audio/yes/db79a764_nohash_0.wav\n",
       "2   yes         0  c6a23ff5  ./data/train/audio/yes/c6a23ff5_nohash_0.wav\n",
       "3   yes         0  c93d5e22  ./data/train/audio/yes/c93d5e22_nohash_2.wav\n",
       "4   yes         0  b575b5fb  ./data/train/audio/yes/b575b5fb_nohash_0.wav"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unknown    36818\n",
       "stop        2134\n",
       "yes         2116\n",
       "up          2115\n",
       "go          2112\n",
       "right       2111\n",
       "on          2110\n",
       "left        2106\n",
       "no          2105\n",
       "off         2101\n",
       "down        2095\n",
       "silence        6\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "silence_files = train_df[train_df.label == 'silence']\n",
    "train_df      = train_df[train_df.label != 'silence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_wav_file(fname):\n",
    "    _, wav = wavfile.read(fname)\n",
    "    wav = wav.astype(np.float32) / np.iinfo(np.int16).max\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    }
   ],
   "source": [
    "silence_data = np.concatenate([read_wav_file(x) for x in silence_files.wav_file.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import stft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_wav_file(fname):\n",
    "    wav = read_wav_file(fname)\n",
    "    \n",
    "    L = 16000  # 1 sec\n",
    "    \n",
    "    if len(wav) > L:\n",
    "        i = np.random.randint(0, len(wav) - L)\n",
    "        wav = wav[i:(i+L)]\n",
    "    elif len(wav) < L:\n",
    "        rem_len = L - len(wav)\n",
    "        i = np.random.randint(0, len(silence_data) - rem_len)\n",
    "        silence_part = silence_data[i:(i+L)]\n",
    "        j = np.random.randint(0, rem_len)\n",
    "        silence_part_left  = silence_part[0:j]\n",
    "        silence_part_right = silence_part[j:rem_len]\n",
    "        wav = np.concatenate([silence_part_left, wav, silence_part_right])\n",
    "    \n",
    "    specgram = stft(wav, 16000, nperseg = 400, noverlap = 240, nfft = 512, padded = False, boundary = None)\n",
    "    phase = np.angle(specgram[2]) / np.pi\n",
    "    amp = np.log1p(np.abs(specgram[2]))\n",
    "    \n",
    "    return np.stack([phase, amp], axis = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weights file\n",
    "model_file = './weights/model.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Conv2D, MaxPooling2D, Activation, BatchNormalization, GlobalAveragePooling2D, GlobalMaxPool2D, concatenate, Dense, Dropout\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_generator(train_batch_size):\n",
    "    while True:\n",
    "        this_train = train_df.groupby('label_id').apply(lambda x: x.sample(n = 2000))\n",
    "        shuffled_ids = random.sample(range(this_train.shape[0]), this_train.shape[0])\n",
    "        for start in range(0, len(shuffled_ids), train_batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + train_batch_size, len(shuffled_ids))\n",
    "            i_train_batch = shuffled_ids[start:end]\n",
    "            for i in i_train_batch:\n",
    "                x_batch.append(process_wav_file(this_train.wav_file.values[i]))\n",
    "                y_batch.append(this_train.label_id.values[i])\n",
    "            x_batch = np.array(x_batch)\n",
    "            y_batch = to_categorical(y_batch, num_classes = len(POSSIBLE_LABELS))\n",
    "            yield x_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid_generator(val_batch_size):\n",
    "    while True:\n",
    "        ids = list(range(valid_df.shape[0]))\n",
    "        for start in range(0, len(ids), val_batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + val_batch_size, len(ids))\n",
    "            i_val_batch = ids[start:end]\n",
    "            for i in i_val_batch:\n",
    "                x_batch.append(process_wav_file(valid_df.wav_file.values[i]))\n",
    "                y_batch.append(valid_df.label_id.values[i])\n",
    "            x_batch = np.array(x_batch)\n",
    "            y_batch = to_categorical(y_batch, num_classes = len(POSSIBLE_LABELS))\n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\" return keras model \"\"\"\n",
    "    x_in = Input(shape = (257,98,2))\n",
    "    x = BatchNormalization()(x_in)\n",
    "    for i in range(4):\n",
    "        x = Conv2D(16*(2 ** i), (3,3))(x)\n",
    "        x = Activation('elu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(128, (1,1))(x)\n",
    "    x_branch_1 = GlobalAveragePooling2D()(x)\n",
    "    x_branch_2 = GlobalMaxPool2D()(x)\n",
    "    x = concatenate([x_branch_1, x_branch_2])\n",
    "    x = Dense(256, activation = 'relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(len(POSSIBLE_LABELS), activation = 'softmax')(x)\n",
    "    model = Model(inputs = x_in, outputs = x)\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 257, 98, 2)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 257, 98, 2)    8           input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 255, 96, 16)   304         batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 255, 96, 16)   0           conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 255, 96, 16)   64          activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)   (None, 127, 48, 16)   0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 125, 46, 32)   4640        max_pooling2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 125, 46, 32)   0           conv2d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 125, 46, 32)   128         activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D)  (None, 62, 23, 32)    0           batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)               (None, 60, 21, 64)    18496       max_pooling2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 60, 21, 64)    0           conv2d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, 60, 21, 64)    256         activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D)  (None, 30, 10, 64)    0           batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)               (None, 28, 8, 128)    73856       max_pooling2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 28, 8, 128)    0           conv2d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNor (None, 28, 8, 128)    512         activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D)  (None, 14, 4, 128)    0           batch_normalization_15[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)               (None, 14, 4, 128)    16512       max_pooling2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glob (None, 128)           0           conv2d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling2d_3 (GlobalMa (None, 128)           0           conv2d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 256)           0           global_average_pooling2d_3[0][0] \n",
      "                                                                   global_max_pooling2d_3[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 256)           65792       concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 256)           0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 12)            3084        dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 183,652\n",
      "Trainable params: 183,168\n",
      "Non-trainable params: 484\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./weights/model.hdf5\n"
     ]
    }
   ],
   "source": [
    "print('Loading model from {}'.format(model_file))\n",
    "model.load_weights(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                           patience=5,\n",
    "                           verbose=1,\n",
    "                           min_delta=0.01,\n",
    "                           mode='min'),\n",
    "             ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=3,\n",
    "                               verbose=1,\n",
    "                               epsilon=0.01,\n",
    "                               mode='min'),\n",
    "             ModelCheckpoint(monitor='val_loss',\n",
    "                             filepath=model_file,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode='min')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "344/344 [==============================] - 1469s - loss: 0.4033 - acc: 0.8753 - val_loss: 0.6896 - val_acc: 0.7763\n",
      "Epoch 2/12\n",
      "344/344 [==============================] - 1342s - loss: 0.3641 - acc: 0.8860 - val_loss: 0.4781 - val_acc: 0.8580\n",
      "Epoch 3/12\n",
      "344/344 [==============================] - 1356s - loss: 0.3431 - acc: 0.8929 - val_loss: 0.3973 - val_acc: 0.8754\n",
      "Epoch 4/12\n",
      "344/344 [==============================] - 1357s - loss: 0.3203 - acc: 0.9009 - val_loss: 0.7732 - val_acc: 0.7451\n",
      "Epoch 5/12\n",
      "344/344 [==============================] - 1358s - loss: 0.3164 - acc: 0.9011 - val_loss: 0.6407 - val_acc: 0.8049\n",
      "Epoch 6/12\n",
      "344/344 [==============================] - 1354s - loss: 0.2949 - acc: 0.9107 - val_loss: 0.6474 - val_acc: 0.8099\n",
      "Epoch 7/12\n",
      "343/344 [============================>.] - ETA: 3s - loss: 0.2808 - acc: 0.9153\n",
      "Epoch 00006: reducing learning rate to 0.00010000000475.\n",
      "344/344 [==============================] - 1368s - loss: 0.2813 - acc: 0.9152 - val_loss: 0.8691 - val_acc: 0.7733\n",
      "Epoch 8/12\n",
      "344/344 [==============================] - 1359s - loss: 0.1827 - acc: 0.9435 - val_loss: 0.4325 - val_acc: 0.8822\n",
      "Epoch 9/12\n",
      "344/344 [==============================] - 1354s - loss: 0.1619 - acc: 0.9496 - val_loss: 0.4013 - val_acc: 0.8931\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7f43b483b990>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 12 epochs ont ete faite sur model.hdf5\n",
    "\n",
    "model.fit_generator(generator=train_generator(64),\n",
    "                              steps_per_epoch=344,\n",
    "                              epochs=12,\n",
    "                              verbose=1,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=valid_generator(64),\n",
    "                              validation_steps=int(np.ceil(valid_df.shape[0]/64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6464/6464 [==============================] - 97s    \n",
      "\n",
      "Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.82      0.88       261\n",
      "          1       0.35      0.97      0.51       270\n",
      "          2       0.00      0.00      0.00         0\n",
      "          3       0.57      0.89      0.69       264\n",
      "          4       0.70      0.86      0.77       247\n",
      "          5       0.46      0.90      0.61       256\n",
      "          6       0.88      0.76      0.82       257\n",
      "          7       0.76      0.78      0.77       256\n",
      "          8       0.78      0.74      0.76       246\n",
      "          9       0.74      0.47      0.57       260\n",
      "         11       0.98      0.68      0.80      4147\n",
      "\n",
      "avg / total       0.87      0.72      0.77      6464\n",
      "\n",
      "\n",
      "Confusion matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAEWCAYAAAC6xlbpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5MHZOefq0P8Dut6FrKDkKeYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9fa2991090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def generator_to_subset(gen, size):\n",
    "    \"\"\" Return a ndarray of size elements for generator gen (gen yields mini batches (X, y)) \"\"\"\n",
    "    X, Y = next(gen)\n",
    "    for _ in range(size):\n",
    "        x, y = next(gen)\n",
    "        X = np.concatenate((X, x), axis=0)\n",
    "        Y = np.concatenate((Y, y), axis=0)\n",
    "    return X, Y\n",
    "\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Confusion matrix of the classifier')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    \n",
    "    # Force multi labels\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(1))\n",
    "    \n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "test_set, test_labels = generator_to_subset(valid_generator(64), 100) # 6798 validation instances\n",
    "test_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Predicting classes\n",
    "y_pred = model.predict(test_set, batch_size=64, verbose=1)\n",
    "#target_names = ['unknown', 'stop', 'yes', 'up', 'go', 'right', 'on', 'left', 'no', 'off', 'down', 'silence']\n",
    "predicted_classes = np.argmax(y_pred, axis=1)\n",
    "   \n",
    "print(\"\\nReport\")\n",
    "print(classification_report(test_labels, predicted_classes)) #, target_names=target_names, digits = 3))\n",
    "      \n",
    "print(\"\\nConfusion matrix\")    \n",
    "cm = confusion_matrix(test_labels, predicted_classes)\n",
    "plot_confusion_matrix(cm, target_names)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_paths = glob(os.path.join('./data/', 'test/audio/*wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_generator(test_batch_size):\n",
    "    while True:\n",
    "        for start in range(0, len(test_paths), test_batch_size):\n",
    "            x_batch = []\n",
    "            end = min(start + test_batch_size, len(test_paths))\n",
    "            this_paths = test_paths[start:end]\n",
    "            for x in this_paths:\n",
    "                x_batch.append(process_wav_file(x))\n",
    "            x_batch = np.array(x_batch)\n",
    "            yield x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(test_generator(64), steps=int(np.ceil(len(test_paths)/64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# last batch will contain padding, so remove duplicates\n",
    "submission = dict()\n",
    "for i in range(len(test_paths)):\n",
    "    fname, label = os.path.basename(test_paths[i]), id2name[classes[i]]\n",
    "    submission[fname] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('submission.csv', 'w') as fout:\n",
    "    fout.write('fname,label\\n')\n",
    "    for fname, label in submission.items():\n",
    "        fout.write('{},{}\\n'.format(fname, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
